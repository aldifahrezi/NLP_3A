{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def parse_aspect_node(aspect_node):\n",
    "    category = aspect_node.get('category')\n",
    "    polarity = aspect_node.get('polarity')\n",
    "    \n",
    "    typo_polarity_map = {\n",
    "        'POSITIVE': 'POSITIVE',\n",
    "        'NEGATIVE': 'NEGATIVE',\n",
    "        'NEATIVE': 'NEGATIVE',\n",
    "        'NEGTIVE': 'NEGATIVE',\n",
    "        ' NEGATIVE ': 'NEGATIVE',\n",
    "        'NEGATIVE ': 'NEGATIVE',\n",
    "        'POSITIVETIVE': 'POSITIVE',\n",
    "        'POSITUVE': 'POSITIVE'\n",
    "    }\n",
    "    \n",
    "    polarity = typo_polarity_map[polarity]\n",
    "    \n",
    "    return {category: polarity}    \n",
    "\n",
    "def parse_aspects_node(aspects_node):\n",
    "    default_aspects = {\n",
    "        'FOOD': 'NEUTRAL',\n",
    "        'AMBIENCE': 'NEUTRAL',\n",
    "        'SERVICE': 'NEUTRAL',\n",
    "        'PRICE': 'NEUTRAL'\n",
    "    }\n",
    "    \n",
    "    for aspect in aspects_node.getchildren():\n",
    "        default_aspects.update(parse_aspect_node(aspect))\n",
    "    \n",
    "    return default_aspects\n",
    "\n",
    "def parse_review_node(review_node):\n",
    "    text = review_node.find('text').text\n",
    "    rid = review_node.get('rid')\n",
    "    aspects = review_node.findall('aspects')\n",
    "    \n",
    "    default_dict = {\n",
    "        'rid': int(rid),\n",
    "        'text': text\n",
    "    }\n",
    "    \n",
    "    res = []\n",
    "    for aspect in aspects:\n",
    "        cur_dict = default_dict.copy()\n",
    "        cur_dict.update(parse_aspects_node(aspect))\n",
    "        res.append(cur_dict)\n",
    "        \n",
    "    return res\n",
    "\n",
    "def filter_same_train_aspects(reviews):\n",
    "    res = []\n",
    "    for v in reviews:\n",
    "        if len(v['aspects']) == 1 or v['aspects'][0] == v['aspects'][1]:\n",
    "            res.append(v)\n",
    "    \n",
    "    return res\n",
    "\n",
    "def filter_different_train_aspects(reviews):\n",
    "    res = []\n",
    "    for v in reviews:\n",
    "        if len(v['aspects']) == 2 and not(v['aspects'][0] == v['aspects'][1]):\n",
    "            res.append(v)\n",
    "            \n",
    "    return res\n",
    "\n",
    "def parse_dataset(filename):\n",
    "    root_node = xml.etree.ElementTree.parse(filename).getroot()\n",
    "    review_nodes = root_node.findall('review')\n",
    "    reviews = [item for sublist in review_nodes for item in parse_review_node(sublist)]\n",
    "    \n",
    "    return pd.DataFrame.from_dict(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_parsed = parse_dataset('../training_set.xml')\n",
    "validation_parsed = parse_dataset('../validation_set.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_zomato_reviews():\n",
    "    with open('../scrapper/reviews.json', 'r') as fp:\n",
    "        reviews = json.load(fp)['reviews']\n",
    "    \n",
    "    sentences_tokens = []\n",
    "\n",
    "    for review in reviews:\n",
    "        try :\n",
    "            tokens =  re.sub(r\"[^a-z0-9]+\", \" \", review.lower()).split()\n",
    "            sentences_tokens.append(tokens)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return sentences_tokens\n",
    "\n",
    "def tokenize_dataset(res):\n",
    "    sentences_tokens=[]\n",
    "    \n",
    "    for id in np.unique(res.rid.values):\n",
    "        df = res[res.rid == id]\n",
    "        text = df.iloc[0]['text']\n",
    "        tokens =  re.sub(r\"[^a-z0-9]+\", \" \", text.lower()).split()\n",
    "        sentences_tokens.append(tokens)\n",
    "        \n",
    "    return sentences_tokens\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrap_tokenize = tokenize_zomato_reviews()\n",
    "# test_tokenize = tokenize_dataset(training_parsed)\n",
    "# validation_tokenize = tokenize_dataset(validation_parsed)\n",
    "\n",
    "# all_tokenize = []\n",
    "# all_tokenize.extend(scrap_tokenize)\n",
    "# all_tokenize.extend(test_tokenize)\n",
    "# all_tokenize.extend(validation_tokenize)\n",
    "\n",
    "# model = Word2Vec(\n",
    "#     sentences=all_tokenize,\n",
    "#     size=100,\n",
    "#     window=5,\n",
    "#     min_count=3,\n",
    "#     workers=4,\n",
    "# )\n",
    "\n",
    "# with open('wordmodel', 'wb') as fp:\n",
    "#     pickle.dump(model, fp, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.89402383e-02   1.21436203e+00   1.24927247e+00   3.71148556e-01\n",
      "  -6.13080680e-01   6.43768013e-01   1.18003391e-01   1.99595630e-01\n",
      "   1.40739167e+00   1.77181530e+00  -7.15958059e-01   3.79979283e-01\n",
      "   4.35211122e-01  -3.36454995e-02  -5.97849429e-01  -5.31703830e-01\n",
      "  -8.82247463e-02   1.12970367e-01  -1.06464863e+00   1.00283647e+00\n",
      "  -1.19164765e+00  -7.93579102e-01   6.59804404e-01  -3.46191764e-01\n",
      "   1.68263521e-02  -1.90282211e-01  -9.12232161e-01   3.48721176e-01\n",
      "   1.29581869e-01  -1.73281774e-01  -8.37333679e-01  -9.02165055e-01\n",
      "   2.88223118e-01  -3.87077481e-01   4.46943253e-01  -3.52096319e-01\n",
      "  -7.94674993e-01  -1.84090805e+00  -8.73749435e-01   9.36676443e-01\n",
      "   1.50449514e-01   1.33542359e+00   2.13218597e-03  -5.79675078e-01\n",
      "   4.26380366e-01  -3.54437739e-01   2.51761413e+00  -8.01229179e-01\n",
      "   2.08790705e-01  -1.22829750e-01   9.83762443e-01   5.07659554e-01\n",
      "  -1.62639201e+00   5.79870641e-01  -1.75448060e+00  -9.09258068e-01\n",
      "  -3.66892934e-01   2.20756078e+00   2.81835288e-01   5.48448563e-01\n",
      "   6.82737648e-01  -7.76014626e-01  -1.38268828e-01   6.88853800e-01\n",
      "   1.27380240e+00  -4.01879132e-01   7.03859150e-01  -1.87482879e-01\n",
      "   5.83986521e-01  -1.08521044e+00  -5.12862392e-03  -2.66294241e-01\n",
      "  -1.64197117e-01  -3.80730778e-01   6.98599398e-01  -3.95232797e-01\n",
      "   1.04389775e+00   4.50033471e-02   1.25997150e+00   1.63222873e+00\n",
      "   1.68033326e+00  -6.73244715e-01  -5.34771681e-01  -8.91357362e-02\n",
      "  -2.81467382e-02   9.33956981e-01  -1.08792269e+00  -1.08142543e+00\n",
      "  -1.17243755e+00  -6.27385318e-01   6.36921346e-01  -4.04523998e-01\n",
      "   2.98621684e-01  -1.24690723e+00   4.10992563e-01   1.28350747e+00\n",
      "  -1.65791476e+00  -7.67541170e-01   1.75039440e-01   1.10842597e+00]\n"
     ]
    }
   ],
   "source": [
    "with open('wordmodel', 'rb') as fp:\n",
    "    model = pickle.load(fp)\n",
    "    \n",
    "print(model.wv['this'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
