{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def parse_aspect_node(aspect_node):\n",
    "    category = aspect_node.get('category')\n",
    "    polarity = aspect_node.get('polarity')\n",
    "    \n",
    "    typo_polarity_map = {\n",
    "        'POSITIVE': 'POSITIVE',\n",
    "        'NEGATIVE': 'NEGATIVE',\n",
    "        'NEATIVE': 'NEGATIVE',\n",
    "        'NEGTIVE': 'NEGATIVE',\n",
    "        ' NEGATIVE ': 'NEGATIVE',\n",
    "        'NEGATIVE ': 'NEGATIVE',\n",
    "        'POSITIVETIVE': 'POSITIVE',\n",
    "        'POSITUVE': 'POSITIVE'\n",
    "    }\n",
    "    \n",
    "    polarity = typo_polarity_map[polarity]\n",
    "    \n",
    "    return {category: polarity}    \n",
    "\n",
    "def parse_aspects_node(aspects_node):\n",
    "    default_aspects = {\n",
    "        'FOOD': 'NEUTRAL',\n",
    "        'AMBIENCE': 'NEUTRAL',\n",
    "        'SERVICE': 'NEUTRAL',\n",
    "        'PRICE': 'NEUTRAL'\n",
    "    }\n",
    "    \n",
    "    for aspect in aspects_node.getchildren():\n",
    "        default_aspects.update(parse_aspect_node(aspect))\n",
    "    \n",
    "    return default_aspects\n",
    "\n",
    "def parse_review_node(review_node):\n",
    "    text = review_node.find('text').text\n",
    "    rid = review_node.get('rid')\n",
    "    aspects = review_node.findall('aspects')\n",
    "    \n",
    "    default_dict = {\n",
    "        'rid': int(rid),\n",
    "        'text': text\n",
    "    }\n",
    "    \n",
    "    res = []\n",
    "    for aspect in aspects:\n",
    "        cur_dict = default_dict.copy()\n",
    "        cur_dict.update(parse_aspects_node(aspect))\n",
    "        res.append(cur_dict)\n",
    "        \n",
    "    return res\n",
    "\n",
    "def filter_same_train_aspects(reviews):\n",
    "    res = []\n",
    "    for v in reviews:\n",
    "        if len(v['aspects']) == 1 or v['aspects'][0] == v['aspects'][1]:\n",
    "            res.append(v)\n",
    "    \n",
    "    return res\n",
    "\n",
    "def filter_different_train_aspects(reviews):\n",
    "    res = []\n",
    "    for v in reviews:\n",
    "        if len(v['aspects']) == 2 and not(v['aspects'][0] == v['aspects'][1]):\n",
    "            res.append(v)\n",
    "            \n",
    "    return res\n",
    "\n",
    "def parse_dataset(filename):\n",
    "    root_node = xml.etree.ElementTree.parse(filename).getroot()\n",
    "    review_nodes = root_node.findall('review')\n",
    "    reviews = [item for sublist in review_nodes for item in parse_review_node(sublist)]\n",
    "    \n",
    "    return pd.DataFrame.from_dict(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_parsed = parse_dataset('../training_set.xml')\n",
    "validation_parsed = parse_dataset('../validation_set.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_zomato_reviews():\n",
    "    with open('../scrapper/reviews.json', 'r') as fp:\n",
    "        reviews = json.load(fp)['reviews']\n",
    "    \n",
    "    sentences_tokens = []\n",
    "\n",
    "    for review in reviews:\n",
    "        try :\n",
    "            tokens =  re.sub(r\"[^a-z0-9]+\", \" \", review.lower()).split()\n",
    "            sentences_tokens.append(tokens)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return sentences_tokens\n",
    "\n",
    "def tokenize_dataset(res):\n",
    "    sentences_tokens=[]\n",
    "    \n",
    "    for id in np.unique(res.rid.values):\n",
    "        df = res[res.rid == id]\n",
    "        text = df.iloc[0]['text']\n",
    "        tokens =  re.sub(r\"[^a-z0-9]+\", \" \", text.lower()).split()\n",
    "        sentences_tokens.append(tokens)\n",
    "        break\n",
    "        \n",
    "    return sentences_tokens\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ini dia toko lawas jaman bapak gua masih ABG kayanya. Namanya emang agak aneh, cocok buat turis2 yg lg cari makan enak dan oleh2. Letaknya di Jl. Sangaji deket daerah harmoni  Kenapa cocok buat wisatawan? Karena disini dijual macem2 oleh2 dari mana aja hampir ada disini. Kalo ga salah ada sambal bu rudy, dll.  Untuk jajanan yg juara ya otak-otaknya. Satu otak2 harga Rp.8.000 tapi dijamin, makan 4 biji udah bisa kenyaang..  Disini juga ada asinan sama juhi. Kemarin makan disini juhinya emang rasa jadul banget, tapi masih kalah enak sama jugi di Jl. Sabang sih hehe (fyi juhi per porsi Rp. 20.000)  Dan es kopyor kalau gak salah harganya 16.000\n",
      " Iseng banget kesini sama temen karena udah lama pengen kesini . Dan aku ngerti kenapa tempat ini hype banget . Suasananya enak , bagus buat foto2 , dan MAKANANNYA ENAK SEMUA ! ! ! ! Aku dan temenku kalap banget beli semua makanan . Waitressnya baik semua juga . Pesen ayam keranjang yang enak banget itu , sate ayam , roti goreng , dan ubi crispy buat makanannya . Overall makanan enak semua . Minumannya pun namanya lucu2 . Aku pesen pelangi di matamu . Very highly recommended buat yang mau ngobrol santai cantik .\n",
      "1029 0 0 3\n"
     ]
    }
   ],
   "source": [
    "scrap_tokenize = tokenize_zomato_reviews()\n",
    "test_tokenize = tokenize_dataset(training_parsed)\n",
    "validation_tokenize = tokenize_dataset(validation_parsed)\n",
    "\n",
    "all_tokenize = []\n",
    "all_tokenize.append(scrap_tokenize)\n",
    "all_tokenize.append(test_tokenize)\n",
    "all_tokenize.append(validation_tokenize)\n",
    "\n",
    "print(len(scrap_tokenize), len(test_tokenize), len(validation_tokenize), len(all_tokenize))\n",
    "# model = Word2Vec(\n",
    "#     sentences=sentences_tokens,\n",
    "#     size=100,\n",
    "#     window=5,\n",
    "#     min_count=1,\n",
    "#     workers=4,\n",
    "# )\n",
    "\n",
    "# with open('wordmodel', 'wb') as fp:\n",
    "#     pickle.dump(model, fp, pickle.HIGHEST_PROTOCOL)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
